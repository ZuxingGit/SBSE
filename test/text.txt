1	Introduction
1.1	Motivation
Language modelling is a widely used technique in Statistical Machine Translation (SMT) [26] and Natural Language Processing (NLP) [24]. After training a language model on a large corpus of code, it then shows that previously seen code has high naturalness, while unseen code has lower naturalness. To figure out what the ‘naturalness’ of software code means, whether it means repetitiveness and predictability, researchers need to conduct necessary preprocessing on code of different programming languages. Similar to NLP methods, different tasks require different preprocessing methods, e.g. tuning or cleaning. If someone wants to develop a tool for English grammar detection and improvement, stopwords such as ‘in, on, the, of, this’ should be kept. If extracting topics from articles is the goal, then stopwords could be deleted since they will introduce noise and make predictions less accurate. If aiming to spot syntax errors, then SyntaxTokens must be kept. On the contrary, if recommending possible code snippets is the goal, SyntaxTokens will reduce the prediction chances.
1.2	Relevant Work
Research into language entropy
There is a long history of language entropy measurement for understanding the redundancy of words. Shannon [52] developed a method of statistical measures of English language entropy. After that, Gabel and Su [17] found code is highly redundant. Hindle et al. [21], following this path, verified that programs are very repetitive and predictable. Other works also examined that repetition happens at higher granularity [30], from package, and class to statement, line, and even word level. Repetition has also been studied in Android Apps [4, 29]. All of the research proves that code is repetitive and follows a predictable pattern.
Research on code validation and checking
Static analysis is often used for detecting code defects and faults. Recent works show it is common to use statistical features of languages for bugs detection and patches generating. Campbell et al. [9] discover that n-gram language models are helpful in syntax error detection. Some researchers state bugs normally are less natural on having relatively high entropy [23, 44]. Santos and Hindle [51] present that the cross-entropy of text split into n-grams in commit messages is an effective way of detecting bad commits that can cause build failure.
Research on recommenders and autocompletion
Most IDE tools provide the function of autocompletion that can suggest a next word, or snippets based on the structure of existing code. Researchers must have known code is repetitive before they work on the development of code prediction function. We can give appropriate guidance to coders during their activities by comparing the similarity rate of written code [3], commit marks [11], besides API usages [33].  We can recommend relevant source code by using association rule mining on the CVS repository [58]. Azad et al. [4] have been able to generate reliable change rule predictions from related Apps and the code snippets on StackOverflow. Sophisticated recommendations can be achieved by combining the history of code and the repetitiveness of programs to generate code components for coders. Buse and Weimer [7] succeed in generating automatic code recommendations from a large corpus of using-API code. Nguyen et al. [39] managed to recommend accurate snippets by using statistical language models. The following work of Nguyen and Nguyen [35] can create syntactically valid recommendations by adopting graph representations.
Research on statistical translation
Google Translate is a good example of translating English to code by copying the successful mechanism of Statistical Machine Translation in natural languages. SWIM [42] can generate API sample snippets through a query corpus from Bing to align English and code. DeepAPI [19] trains recurrent neural networks on aligned source code comments with code for translating longer sequences of API calls. T2API [37, 38] can generate a set of API calls as well as producing extensive graphs of common API usages by aligning English text with code from StackOverflow posts. By comparison, T2API can produce more complex usages by re-ordering API elements, while SWIM and DeepAPI only produce short and simplistic code sequences because of their left-to-right preprocessing of code.
2	Summary & Compare
The research of this article mainly focuses on proving the repetitiveness and predictability of API usages, which obviously supports API recommendation approaches. It also shows graphs are a more appropriate representation of programming languages. They used a large corpus of 7 programming languages from GitHub.  Necessary and sufficient comparisons were made during the research process. Table 1 below shows the proportions of tokens of each programming language.
 
Table 1: Corpus size in tokens. The total of tokens is held constant across languages.

The naturalness of source code stems from language-specific syntax, particularly separators and semicolons, which make up 44% of all tokens in a Java corpus. Although code remains repetitive and predictable (to a lesser extent) even after removing punctuation and stopwords. They adopt Hindle et al.’s [21] equation for calculating cross-entropy of a document D in a language model M:
 
Instead of developing the language model M from other documents and corpus, they divide a single corpus into 10 folds. Model is trained from 9 of the folds and H (D, M) is calculated with the remaining fold of D. By calculating the average value of all folds, we get the final SelfCrossEntropy. Lower SelfCrossEntropy means higher repetitiveness. As shown below in Figure 2, scala is much less repetitive compared with C#. 
 
Programming languages become less repetitive when SyntaxTokens are removed. Technical discussions in English on StackOverflow exhibit a similar degree of repetition as code.
API code usages are highly repetitive because certain API calls, such as opening and closing files, are common across various programs and projects. This graph suggests that raw Java code, which includes SyntaxTokens, is more repetitive than Java API usages, likely due to the frequent application of syntax rules. 
Fig. 3: Comparison of SelfCrossEntropy.

Applications can leverage this feature to recommend sophisticated API call sequences using reliable statistical models. Although n-gram models were developed for natural languages, they are not well-suited for representing code, which is typically structured as an Abstract Syntax Tree (AST). AST contains control and data flow. By comparison, statistical graph representations can extract higher-level patterns than n-grams. It can even capture complex coding patterns. For example, the structure of iteration through a HashMap, as the Figure below shows.
 
Fig. 5. A GROUM representing iteration on a HashMap.
3	Future Suggestion
The outcomes of this research paper are obviously correct and convincing. However, some flaws exist in their recommended graph model. For example, GROUM can not generate graph presentations successfully if one node is missing. Future works should try to solve this problem to make it more robust. Secondly, the computational expense of identifying isomorphic or similar graphs is relatively higher, but fewer nodes are needed to represent the same block of code compared to sequential n-grams. Higher n-node graphs exhibit similar degrees of repetition as 3-node and 4-node graphs. SelfCrossEntropy has the limitation of being impractical to calculate all possible combinations of the next token due to its complexity, which is O(t^n), where t represents the number of unique tokens and n represents the total number of tokens in the corpus. In practice, for each chosen language, there are over 300,000 unique tokens and 20 million total tokens.
To summarise, the GROUM is not reliable enough now. It needs some improvements in the future. The model is a bit time-consuming even though it works as a good approximation of entropy. Future research can focus on how to optimize the efficiency of the calculation or simplify the process.
4	Critical Review
4.1	Strengths
They created sufficient evidence that proves that source code is repetitive and predictable using projects of 7 programming languages (i.e. Java, C#, C, JavaScript, Python, Ruby, Scala) picked from GitHub. 134 open-source projects are included in the code corpus. For comparison, they also selected two kinds of English corpus: Gutenberg corpus, and StackOverflow posts. The contribution of SyntaxTokens to repetition was confirmed by the comparison of SelfCrossEntropy between the original code and the code without SyntaxTokens. Furthermore, API usage is also a factor that makes the code repetitive. By comparing the repetitiveness of n-grams and graph representations of code, it is evident that graph representations are more repetitive than sequential ones. Consequently, they are more suitable for code recommendation tasks because they can capture non-sequential and complex relationships within the code.
4.2	Weaknesses
There are not many bad or not-good-enough things about this research. What it states are just some plain truths that are basically the same as our guess. If I have to say something not good, it’s this research did not surprise me. I want to learn something and get some suggestions about how students can avoid the high similarity rate of their code if they use code recommendation or autocompletion applications. This research only says that a graph is more appropriate than an n-gram model in code prediction.
4.3	My method
No extra method is needed for this paper. They have done very good research on this area even though the results are not very surprising but at least is inspiring. They offered their GitHub repository containing scripts and output data for future researchers to reproduce and check the results, or reuse for new relevant research.
References

Rahman, M, Palani, D & Rigby, PC 2019, ‘Natural Software Revisited’, in 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), IEEE, pp. 37–48.

